import logging
import os
from pathlib import Path
import sys
from typing import List, Optional

from llama_index.core import (
                        Document,
                        PromptTemplate,
                        Settings,
                        StorageContext,
                        QueryBundle,
                        VectorStoreIndex,
                        )
from llama_index.core.indices.postprocessor import (
                        SentenceTransformerRerank,
                        MetadataReplacementPostProcessor,
                        )
from llama_index.core.node_parser import (
                        SentenceWindowNodeParser,
                        SentenceSplitter,
                        )
from llama_index.core.postprocessor.types import BaseNodePostprocessor
from llama_index.core.retrievers import QueryFusionRetriever
from llama_index.core.schema import NodeWithScore
from llama_index.core.query_engine import RetrieverQueryEngine

from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.readers.file import (
                        PDFReader,
                        PyMuPDFReader,
                        )
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.storage.docstore.mongodb import MongoDocumentStore
from llama_index.vector_stores.milvus import MilvusVectorStore

from pymilvus import connections, db, MilvusClient
import openai

from pymongo import MongoClient
from trulens_eval import Tru
from utility import (
                change_default_engine_prompt_to_in_detail,
                change_tree_engine_prompt_to_in_detail,
                display_prompt_dict,
                get_article_link, 
                get_database_and_sentence_splitter_collection_name,
                get_tree_engine_from_retriever,
                get_tree_engine_with_sort_from_retriever,
                print_retreived_nodes,
                SortNodePostprocessor,
                )
from database_operation import (
                check_if_milvus_database_collection_exist,
                check_if_mongo_database_namespace_exist
                )


# class SortNodePostprocessor(BaseNodePostprocessor):
#     def _postprocess_nodes(
#             self, 
#             nodes: List[NodeWithScore], 
#             query_bundle: Optional[QueryBundle]
#             ) -> List[NodeWithScore]:
        
#         # Custom post-processor: Order nodes based on the order it appears in a document (using "start_char_idx")

#         # Create new node dictionary
#         _nodes_dic = [{"start_char_idx": node.node.start_char_idx, "node": node} for node in nodes]

#         # Sort based on start_char_idx
#         sorted_nodes_dic = sorted(_nodes_dic, key=lambda x: x["start_char_idx"])

#         # Get the new nodes from the sorted node dic
#         sorted_new_nodes = [node["node"] for node in sorted_nodes_dic]

#         return sorted_new_nodes


def load_document_pdf(doc_link):
    loader = PyMuPDFReader()
    docs0 = loader.load(file_path=Path(doc_link))
    docs = Document(text="\n\n".join([doc.text for doc in docs0]))
    # print(type(documents), "\n")
    # print(len(documents), "\n")
    # print(type(documents[0]))
    # print(documents[0])
    return docs

def get_nodes_from_document_sentence_splitter(
        _documnet, 
        _chunk_size,
        _chunk_overlap
        ):
    
    # create the sentence spitter node parser
    node_parser = SentenceSplitter(
                                chunk_size=_chunk_size,
                                chunk_overlap=_chunk_overlap
                                )
    _nodes = node_parser.get_nodes_from_documents([_documnet])

    return _nodes


def build_sentence_splitter_index_and_bm25_docsrore(
    _article_link,
    _save_index,
    _add_document,
    _chunk_size,
    _chunk_overlap
    ):

    if _save_index or _add_document:  # Only load and parse document if either index or docstore not saved.
        _document = load_document_pdf(_article_link)
        _nodes = get_nodes_from_document_sentence_splitter(
            _document, 
            _chunk_size,
            _chunk_overlap
            )
    if _save_index == True:
        # Create and save index (embedding) to Milvus database
        _index = VectorStoreIndex(
            nodes=_nodes,
            storage_context=storage_context,
            )
    else:
        # Load from Milvus database
        _index = VectorStoreIndex.from_vector_store(
            vector_store=vector_store
            )
    if _add_document == True:
        # Save document nodes to Mongodb docstore at the server
        storage_context.docstore.add_documents(_nodes)

    return _index

def get_sentence_splitter_index_bm25_and_fusion_retriever(
        _index,
        _similarity_top_k,
        _num_queries,
        _fusion_top_n,
        ):
    
    _retriever = _index.as_retriever(
        similarity_top_k=_similarity_top_k
        )
    _bm25_retriever = BM25Retriever.from_defaults(
        similarity_top_k=_similarity_top_k,
        docstore=docstore,
        )
    _fusion_retriever = QueryFusionRetriever(
        retrievers=[_retriever, _bm25_retriever],
        similarity_top_k=_fusion_top_n,
        num_queries=_num_queries,  # set this to 1 to disable query generation
        mode="reciprocal_rerank",
        use_async=True,
        verbose=True,
        # query_gen_prompt="...",  # for overriding the query generation prompt
        )
    return _retriever, _bm25_retriever, _fusion_retriever


def get_default_engine_from_retriever(
        _retriever,
        _bm25_retriever,
        _fusion_retriever,
        ):
    """
    default mode is response_mode="compact".
    """

    _default_vector_engine = RetrieverQueryEngine.from_args(
        retriever=_retriever,
        # response_mode="default"  # Unknown mode "default"
        )
    _default_bm25_engine = RetrieverQueryEngine.from_args(
        retriever=_bm25_retriever,
        )
    _default_fusion_engine = RetrieverQueryEngine.from_args(
        retriever=_fusion_retriever,
        )
    
    return _default_vector_engine, _default_bm25_engine, _default_fusion_engine


def get_default_engine_with_sort_from_retriever(
        _retriever,
        _bm25_retriever,
        _fusion_retriever,
        ):
    """
    default mode is response_mode="compact".
    """

    _sort_vector_engine = RetrieverQueryEngine.from_args(
        retriever=_retriever,
        # response_mode="default"  # Unknown mode "default"
        node_postprocessors=[SortNodePostprocessor()],
        )
    _sort_bm25_engine = RetrieverQueryEngine.from_args(
        retriever=_bm25_retriever,
        node_postprocessors=[SortNodePostprocessor()],
        )
    _sort_fusion_engine = RetrieverQueryEngine.from_args(
        retriever=_fusion_retriever,
        node_postprocessors=[SortNodePostprocessor()],
        )

    return _sort_vector_engine, _sort_bm25_engine, _sort_fusion_engine


def get_fusion_refine_tree_and_accumulate_engine_from_retriever(
        _fusion_retriever,
        _similarity_top_k
        ):

    _refine_fusion_engine = RetrieverQueryEngine.from_args(
        retriever=_fusion_retriever, 
        similarity_top_k=_similarity_top_k,
        response_mode="refine",
        )
    _tree_fusion_engine = RetrieverQueryEngine.from_args(
        retriever=_fusion_retriever, 
        similarity_top_k=_similarity_top_k,
        response_mode="tree_summarize",
        )
    _accumulate_fusion_engine = RetrieverQueryEngine.from_args(
        retriever=_fusion_retriever, 
        similarity_top_k=_similarity_top_k,
        response_mode="accumulate",
        )
    
    return _refine_fusion_engine, _tree_fusion_engine, _accumulate_fusion_engine


def get_rerank_refine_tree_and_accumulate_engine_from_retriever(
        _fusion_retriever,
        _rerank_top_n,
        _rerank
        ):
    
    _refine_rerank_engine = RetrieverQueryEngine.from_args(
        retriever=_fusion_retriever, 
        similarity_top_k=_rerank_top_n,
        node_postprocessors=[_rerank],
        response_mode="refine",
        )
    _tree_rerank_engine = RetrieverQueryEngine.from_args(
        retriever=_fusion_retriever, 
        similarity_top_k=_rerank_top_n,
        node_postprocessors=[_rerank],
        response_mode="tree_summarize",
        )
    _accumulate_rerank_engine = RetrieverQueryEngine.from_args(
        retriever=_fusion_retriever, 
        similarity_top_k=_rerank_top_n,
        node_postprocessors=[_rerank],
        response_mode="accumulate",
        )
    
    return _refine_rerank_engine, _tree_rerank_engine, _accumulate_rerank_engine

    
# # Set up logging
# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

# Set OpenAI API key, LLM, and embedding model
openai.api_key = os.environ['OPENAI_API_KEY']

llm = OpenAI(model="gpt-3.5-turbo", temperature=0.1)
Settings.llm = llm

# embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
# Settings.embed_model = embed_model
# embed_model_dim = 384  # for bge-small-en-v1.5
# embed_model_name = "huggingface_embedding_bge_small"

embed_model = OpenAIEmbedding(model_name="text-embedding-3-small")
Settings.embed_model = embed_model
embed_model_dim = 1536  # for text-embedding-3-small
embed_model_name = "openai_embedding_3_small"

# Create article link
article_dictory = "paul_graham"
article_name = "paul_graham_essay.pdf"

article_link = get_article_link(
                                article_dictory,
                                article_name
                                )

# article_dictory = "andrew"
# article_name = "eBook-How-to-Build-a-Career-in-AI.pdf"

# Create database and collection names
chunk_method = "sentence_splitter"
chunk_size = 256
chunk_overlap = 50

(database_name, 
collection_name) = get_database_and_sentence_splitter_collection_name(
                                                            article_dictory, 
                                                            chunk_method, 
                                                            embed_model_name, 
                                                            chunk_size,
                                                            chunk_overlap
                                                            )

# Check if index has already been saved to Milvus database.
# In Milvus database, if the specific collection exists , do not save the index.
# Otherwise, create one.
uri_milvus = "http://localhost:19530"
save_index = check_if_milvus_database_collection_exist(uri_milvus, 
                                                       database_name, 
                                                       collection_name)

# Check if MongoDB already has the namespace
# In MongoDB, if the specific namespace exists, do not add document nodes to MongoDB.
uri_mongo = "mongodb://localhost:27017/"
add_document = check_if_mongo_database_namespace_exist(uri_mongo, 
                                                       database_name, 
                                                       collection_name)

# Initiate vector store (a new empty collection will be created in Milvus server)
vector_store = MilvusVectorStore(
    uri=uri_milvus,
    db_name=database_name,
    collection_name=collection_name,
    dim=embed_model_dim,  # dim of HuggingFace "BAAI/bge-small-en-v1.5" embedding model
    )

# Initiate MongoDB docstore (Not yet save to MongoDB server)
docstore = MongoDocumentStore.from_uri(
    uri=uri_mongo,
    db_name=database_name,
    namespace=collection_name
    )

# Initiate storage context: use Milvus as vector store and Mongo as docstore 
storage_context = StorageContext.from_defaults(
    vector_store=vector_store,
    docstore=docstore
    )

# for i in list(storage_context.docstore.get_all_ref_doc_info().keys()):
#     print(i)
# print(storage_context.docstore.get_node(leaf_nodes[0].node_id))


# Get index (load and parse documents if necessary) and build docstore
index = build_sentence_splitter_index_and_bm25_docsrore(
                                                article_link,
                                                save_index,
                                                add_document,
                                                chunk_size,
                                                chunk_overlap
                                                )

# Get retrievers
similarity_top_k = 12
num_queries = 1  # for QueryFusionRetriever()
fusion_top_n = 5

(vector_retriever, 
bm25_retriever,
fusion_retriever) = get_sentence_splitter_index_bm25_and_fusion_retriever(
                                                            index,
                                                            similarity_top_k,
                                                            num_queries,
                                                            fusion_top_n,
                                                            )

# The code below does not work (cannot put node_postprocessors here)
# window_retriever = index.as_retriever(
#     similarity_top_k=similarity_top_k,
#     node_postprocessors=[postproc],
#     )  

# query_str = "What are the keys to building a career in AI?"
query_str = "What are the things that happen in New York?"
# query_str = "What are the thinkgs happened in New York in detail?"
# query_str = "What happened in New York?"
# query_str = "Describe everything that is mentioned about Interleaf one by one."
# query_str = "Describe everything that is mentioned about Interleaf one by one in detail."
# query_str = "Describe everything that is mentioned about Interleaf."
# query_str = "Describe everything that is mentioned about Viaweb."
# query_str = "What happened at Interleaf?"
# query_str = "What happened at Interleaf and Viaweb?"
# query_str = "What is the importance of networking in AI?"
# query_str = (
#     "What could be the potential outcomes of adjusting the amount of safety"
#     " data used in the RLHF stage?"
# )

# Define window post processor
# postproc = MetadataReplacementPostProcessor(
#     target_metadata_key="window"
#     )

vector_store.client.load_collection(collection_name=collection_name)

# Retrieve nodes, bm25 nodes, and fusion nodes
nodes = vector_retriever.retrieve(query_str)
bm25_nodes = bm25_retriever.retrieve(query_str)
fusion_nodes = fusion_retriever.retrieve(query_str)

# Define reranker
rerank_model = "BAAI/bge-reranker-base"
rerank_top_n = 8
rerank = SentenceTransformerRerank(
    top_n=rerank_top_n,
    model=rerank_model,
    )

# Get re-ranked fusion nodes
rerank_nodes = rerank.postprocess_nodes(
    nodes=fusion_nodes,
    query_str=query_str,
    )

# Print retrieved nodes
print_retreived_nodes("vector", nodes)
print_retreived_nodes("bm25", bm25_nodes)
print_retreived_nodes("fusion", fusion_nodes)
print_retreived_nodes("rerank", rerank_nodes)



# Create default engines (query_mode="compact")
(vector_engine,
 bm25_engine,
 fusion_engine) = get_default_engine_from_retriever(
                                                vector_retriever,
                                                bm25_retriever,
                                                fusion_retriever
 )

# Create default sorted engines (with sort post-processor)
(vector_sort_engine,
 bm25_sort_engine,
 fusion_sort_engine) = get_default_engine_with_sort_from_retriever(
                                                vector_retriever,
                                                bm25_retriever,
                                                fusion_retriever
 )

# Create tree engines (query_mode="tree_summary")
(tree_vector_engine,
 tree_bm25_engine,
 tree_fusion_engine) = get_tree_engine_from_retriever(
                                                vector_retriever,
                                                bm25_retriever,
                                                fusion_retriever
 )

# Create tree sorted engines (with sort post-processor)
(tree_vector_sort_engine,
 tree_bm25_sort_engine,
 tree_fusion_sort_engine) = get_tree_engine_with_sort_from_retriever(
                                                vector_retriever,
                                                bm25_retriever,
                                                fusion_retriever
 )

# # Create sort engine
# sort_engine = RetrieverQueryEngine.from_args(
#     retriever=fusion_retriever, 
#     node_postprocessors=[SortNodePostprocessor()],
#     )

# sort_engine = change_default_engine_prompt_to_in_detail(sort_engine) 
# sort_response = sort_engine.query(query_str)

# print("\nSORTED-FUSION-ENGINE:\n\n" + str(sort_response))
# print(sort_response.get_formatted_sources(length=2000))


# Create default rerank engine (query_mode="compact")
rerank_engine = RetrieverQueryEngine.from_args(
    retriever=fusion_retriever, 
    node_postprocessors=[rerank],
    )

rerank_sort_engine = RetrieverQueryEngine.from_args(
    retriever=fusion_retriever, 
    node_postprocessors=[
        rerank,
        SortNodePostprocessor()
        ],
    )

# Create tree rerank engine (query_mode="compact")
tree_rerank_engine = RetrieverQueryEngine.from_args(
    retriever=fusion_retriever, 
    node_postprocessors=[rerank],
    response_mode="tree_summarize"
    )

tree_rerank_sort_engine = RetrieverQueryEngine.from_args(
    retriever=fusion_retriever, 
    node_postprocessors=[
        rerank,
        SortNodePostprocessor()
        ],
    response_mode="tree_summarize"
    )

# # Create refine, tree, and accumulate fusion engines 
# (refine_fusion_engine,
#  tree_fusion_engine,
#  accumulate_fusion_engine) = get_fusion_refine_tree_and_accumulate_engine_from_retriever(
#                                                                     fusion_retriever,
#                                                                     fusion_top_n,
#                                                                     )

# Change default engine promps to "in detail"
vector_engine = change_default_engine_prompt_to_in_detail(vector_engine) 
bm25_engine = change_default_engine_prompt_to_in_detail(bm25_engine) 
fusion_engine = change_default_engine_prompt_to_in_detail(fusion_engine)
rerank_engine = change_default_engine_prompt_to_in_detail(rerank_engine)

vector_sort_engine = change_default_engine_prompt_to_in_detail(vector_sort_engine) 
bm25_sort_engine = change_default_engine_prompt_to_in_detail(bm25_sort_engine) 
fusion_sort_engine = change_default_engine_prompt_to_in_detail(fusion_sort_engine)
rerank_sort_engine = change_default_engine_prompt_to_in_detail(rerank_sort_engine)

# Change tree engine promps to "in detail"
tree_vector_engine = change_tree_engine_prompt_to_in_detail(tree_vector_engine) 
tree_bm25_engine = change_tree_engine_prompt_to_in_detail(tree_bm25_engine) 
tree_fusion_engine = change_tree_engine_prompt_to_in_detail(tree_fusion_engine) 
tree_rerank_engine = change_tree_engine_prompt_to_in_detail(tree_fusion_engine) 

tree_vector_sort_engine = change_tree_engine_prompt_to_in_detail(tree_vector_sort_engine) 
tree_bm25_sort_engine = change_tree_engine_prompt_to_in_detail(tree_bm25_sort_engine) 
tree_fusion_sort_engine = change_tree_engine_prompt_to_in_detail(tree_fusion_sort_engine) 
tree_rerank_sort_engine = change_tree_engine_prompt_to_in_detail(tree_fusion_sort_engine) 

# # Create refine, tree, and accumulate rerank engines 
# (refine_rerank_engine, 
#  tree_rerank_engine, 
#  accumulate_rerank_engine) = get_rerank_refine_tree_and_accumulate_engine_from_retriever(
#                                                                         fusion_retriever,
#                                                                         rerank_top_n,
#                                                                         rerank
#                                                                         )

# window_prompts_dict = window_engine.get_prompts()
# type = "tree_fusion_engine:"
# display_prompt_dict(type.upper(), window_prompts_dict)

# Get default responses
vector_response = vector_engine.query(query_str)
bm25_response = bm25_engine.query(query_str)
fusion_response = fusion_engine.query(query_str)
rerank_response = rerank_engine.query(query_str)

# Get default sorted responses
vector_sort_response = vector_sort_engine.query(query_str)
bm25_sort_response = bm25_sort_engine.query(query_str)
fusion_sort_response = fusion_sort_engine.query(query_str)
rerank_sort_response = rerank_sort_engine.query(query_str)

# Get tree responses
tree_vector_response = tree_vector_engine.query(query_str)
tree_bm25_response = tree_bm25_engine.query(query_str)
tree_fusion_response = tree_fusion_engine.query(query_str)
tree_rerank_response = tree_rerank_engine.query(query_str)

# Get tree sorted responses
tree_vector_sort_response = tree_vector_sort_engine.query(query_str)
tree_bm25_sort_response = tree_bm25_sort_engine.query(query_str)
tree_fusion_sort_response = tree_fusion_sort_engine.query(query_str)
tree_rerank_sort_response = tree_rerank_sort_engine.query(query_str)

# print(rerank_response.get_formatted_sources(length=2000))

# Prints default responses
print("\nVECTOR-ENGINE:\n\n" + str(vector_response))
print("\nBM25-ENGINE:\n\n" + str(bm25_response))
print("\nFUSION-ENGINE:\n\n" + str(fusion_response))
print("\nRERANK-ENGINE:\n\n" + str(rerank_response))

# Prints default sort responses
print("\nVECTOR-SORT-ENGINE:\n\n" + str(vector_sort_response))
print("\nBM25-SORT-ENGINE:\n\n" + str(bm25_sort_response))
print("\nFUSION-SORT-ENGINE:\n\n" + str(fusion_sort_response))
print("\nRERANK-SORT-ENGINE:\n\n" + str(rerank_sort_response))

# Print tree responses
print("\nTREE-VECTOR-ENGINE:\n\n" + str(tree_vector_response))
print("\nTREE-BM25-ENGINE:\n\n" + str(tree_bm25_response))
print("\nTREE-FUSION-ENGINE:\n\n" + str(tree_fusion_response))
print("\nTREE-RERANK-ENGINE:\n\n" + str(tree_rerank_response))

# Print tree sort responses
print("\nTREE-VECTOR-SORT-ENGINE:\n\n" + str(tree_vector_sort_response))
print("\nTREE-BM25-SORT-ENGINE:\n\n" + str(tree_bm25_sort_response))
print("\nTREE-FUSION-SORT-ENGINE:\n\n" + str(tree_fusion_sort_response))
print("\nTREE-RERANK-SORT-ENGINE:\n\n" + str(tree_rerank_sort_response))

vector_store.client.release_collection(collection_name=collection_name)
vector_store.client.close()  # Need to do this (otherwise Milvus container will hault when closing)
# del docstore  # MongoDB may frnot need to be manually closed.







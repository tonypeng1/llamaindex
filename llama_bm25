import logging
import os
from pathlib import Path
import sys
from typing import List, Optional

from llama_index.core import (
                        Document,
                        Settings,
                        VectorStoreIndex,
                        )
from llama_index.core.indices.postprocessor import (
                        SentenceTransformerRerank,
                        MetadataReplacementPostProcessor,
                        )
from llama_index.core.node_parser import (
                        SentenceSplitter,
                        )
from llama_index.core.query_engine.router_query_engine import RouterQueryEngine
from llama_index.core.retrievers import QueryFusionRetriever
from llama_index.core.schema import NodeWithScore
from llama_index.core.selectors import LLMSingleSelector
from llama_index.core.tools import (
                        QueryEngineTool, 
                        FunctionTool
                        )
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.vector_stores import MetadataFilters

from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.readers.file import (
                        PyMuPDFReader,
                        )
from llama_index.retrievers.bm25 import BM25Retriever

import openai
from trulens_eval import Tru
from utility import (
                change_accumulate_engine_prompt_to_in_detail,
                change_tree_engine_prompt_to_in_detail,
                display_prompt_dict,
                get_article_link, 
                get_bm25_retriever_and_tree_sort_detail_engine,
                get_database_and_sentence_splitter_collection_name,
                get_fusion_retriever_and_tree_sort_detail_engine,
                get_summary_storage_context,
                get_summary_retriever_and_tree_detail_engine,
                get_vector_retriever_and_tree_sort_detail_engine,
                get_vector_store_docstore_and_storage_context,
                PageSortNodePostprocessor,
                print_retreived_nodes,
                SortNodePostprocessor,
                )
from database_operation import (
                check_if_milvus_database_collection_exist,
                check_if_mongo_database_namespace_exist
                )


def load_document_pdf(doc_link):
    loader = PyMuPDFReader()
    docs0 = loader.load(file_path=Path(doc_link))
    # docs = Document(
    #     text="\n\n".join([doc.text for doc in docs0]))
    # print(type(documents), "\n")
    # print(len(documents), "\n")
    # print(type(documents[0]))
    # print(documents[0])
    return docs0

def get_nodes_from_document_sentence_splitter(
        _documnet, 
        _chunk_size,
        _chunk_overlap
        ):
    
    # create the sentence spitter node parser
    node_parser = SentenceSplitter(
                                chunk_size=_chunk_size,
                                chunk_overlap=_chunk_overlap
                                )
    # _nodes = node_parser.get_nodes_from_documents([_documnet])
    _nodes = node_parser.get_nodes_from_documents(_documnet)

    return _nodes


def load_document_nodes_sentence_splitter(
    _article_link,
    _chunk_size,
    _chunk_overlap
    ):
    # Only load and parse document if either index or docstore not saved.
    _document = load_document_pdf(_article_link)
    _nodes = get_nodes_from_document_sentence_splitter(
        _document, 
        _chunk_size,
        _chunk_overlap
        )
    return _nodes


def create_and_save_vector_index_to_milvus_database(_nodes):
    _index = VectorStoreIndex(
        nodes=_nodes,
        storage_context=storage_context_vector,
        )
    return _index


def get_rerank_refine_tree_and_accumulate_engine_from_retriever(
        _fusion_retriever,
        _rerank_top_n,
        _rerank
        ):
    
    _refine_rerank_engine = RetrieverQueryEngine.from_args(
        retriever=_fusion_retriever, 
        similarity_top_k=_rerank_top_n,
        node_postprocessors=[_rerank],
        response_mode="refine",
        )
    _tree_rerank_engine = RetrieverQueryEngine.from_args(
        retriever=_fusion_retriever, 
        similarity_top_k=_rerank_top_n,
        node_postprocessors=[_rerank],
        response_mode="tree_summarize",
        )
    _accumulate_rerank_engine = RetrieverQueryEngine.from_args(
        retriever=_fusion_retriever, 
        similarity_top_k=_rerank_top_n,
        node_postprocessors=[_rerank],
        response_mode="accumulate",
        )
    
    return _refine_rerank_engine, _tree_rerank_engine, _accumulate_rerank_engine


def add(x: int, y: int) -> int:
    """Adds two integers together."""
    return x + y


def mystery(x: int, y: int) -> int: 
    """Mystery function that operates on top of two numbers."""
    return (x + y) * (x + y)
    

# # Set up logging
# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

# Set OpenAI API key, LLM, and embedding model
openai.api_key = os.environ['OPENAI_API_KEY']
llm = OpenAI(model="gpt-3.5-turbo", temperature=0.1)
Settings.llm = llm

# embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
# Settings.embed_model = embed_model
# embed_model_dim = 384  # for bge-small-en-v1.5
# embed_model_name = "huggingface_embedding_bge_small"

embed_model = OpenAIEmbedding(model_name="text-embedding-3-small")
Settings.embed_model = embed_model
embed_model_dim = 1536  # for text-embedding-3-small
embed_model_name = "openai_embedding_3_small"

# Create article link
# article_dictory = "metagpt"
# article_name = "metagpt.pdf"

article_dictory = "paul_graham"
article_name = "paul_graham_essay.pdf"

article_link = get_article_link(article_dictory,
                                article_name
                                )

# article_dictory = "andrew"
# article_name = "eBook-How-to-Build-a-Career-in-AI.pdf"

# Create database and collection names
chunk_method = "sentence_splitter"
chunk_size = 512
chunk_overlap = 128

# chunk_method = "sentence_splitter"
# chunk_size = 256
# chunk_overlap = 50

# Create database name and colleciton names
(database_name, 
collection_name_vector,
collection_name_summary) = get_database_and_sentence_splitter_collection_name(
                                                            article_dictory, 
                                                            chunk_method, 
                                                            embed_model_name, 
                                                            chunk_size,
                                                            chunk_overlap
                                                            )

# Initiate Milvus and MongoDB database
uri_milvus = "http://localhost:19530"
uri_mongo = "mongodb://localhost:27017/"

# Check if the vector index has already been saved to Milvus database.
save_index_vector = check_if_milvus_database_collection_exist(uri_milvus, 
                                                       database_name, 
                                                       collection_name_vector
                                                       )

# Check if the vector document has already been saved to MongoDB database.
add_document_vector = check_if_mongo_database_namespace_exist(uri_mongo, 
                                                       database_name, 
                                                       collection_name_vector
                                                       )

# Check if the summary document has already been saved to MongoDB database.
add_document_summary = check_if_mongo_database_namespace_exist(uri_mongo, 
                                                       database_name, 
                                                       collection_name_summary
                                                       )

# Create vector store, vector docstore, and vector storage context
(vector_store,
 vector_docstore,
 storage_context_vector) = get_vector_store_docstore_and_storage_context(uri_milvus,
                                                                    uri_mongo,
                                                                    database_name,
                                                                    collection_name_vector,
                                                                    embed_model_dim
                                                                    )

# Create summary summary storage context
storage_context_summary = get_summary_storage_context(uri_mongo,
                                                    database_name,
                                                    collection_name_summary
                                                    )

# for i in list(storage_context.docstore.get_all_ref_doc_info().keys()):
#     print(i)
# print(storage_context.docstore.get_node(leaf_nodes[0].node_id))

# Load documnet nodes if either vector index or docstore not saved.
if save_index_vector or add_document_vector or add_document_summary: 
    extracted_nodes = load_document_nodes_sentence_splitter(
                                                    article_link,
                                                    chunk_size,
                                                    chunk_overlap
                                                    )

if save_index_vector == True:
    vector_index = create_and_save_vector_index_to_milvus_database(extracted_nodes)

else:
    # Load from Milvus database
    vector_index = VectorStoreIndex.from_vector_store(
        vector_store=vector_store
        )

if add_document_vector == True:
    # Save document nodes to Mongodb docstore at the server
    storage_context_vector.docstore.add_documents(extracted_nodes)

if add_document_summary == True:
    # Save document nodes to Mongodb docstore at the server
    storage_context_summary.docstore.add_documents(extracted_nodes)

# info = storage_context_summary.docstore.get_all_ref_doc_info()

# Set retriever parameters (based on the search query)
similarity_top_k = 12
num_queries = 1  # for QueryFusionRetriever() in utility.py
fusion_top_n = 8
rerank_top_n = 8

# similarity_top_k = 15
# num_queries = 1  # for QueryFusionRetriever() in utility.py
# fusion_top_n = 8
# rerank_top_n = 8


(vector_retriever,
vector_tree_sort_detail_engine) = get_vector_retriever_and_tree_sort_detail_engine(
                                                                        vector_index,
                                                                        similarity_top_k,
                                                                        )

# vector_filter_engine = vector_index.as_query_engine(
#     similarity_top_k=2,
#     filters=MetadataFilters.from_dicts(
#         [
#             {"key": "source", "value": "2"}
#         ]
#     )
# )

# response = vector_filter_engine.query(
#     "What are some high-level results of MetaGPT?", 
# )



(bm25_retriever,
bm25_tree_sort_detail_engine) = get_bm25_retriever_and_tree_sort_detail_engine(
                                                                        vector_docstore,
                                                                        similarity_top_k,
                                                                        )

(fusion_retriever,
fusion_tree_sort_detail_engine) = get_fusion_retriever_and_tree_sort_detail_engine(
                                                                        vector_index,
                                                                        vector_docstore,
                                                                        similarity_top_k,
                                                                        num_queries,
                                                                        fusion_top_n,
                                                                        )

# Create an accumulate, fusion, and sort engine
accumulate_fusion_sort_engine = RetrieverQueryEngine.from_args(
                                        retriever=fusion_retriever, 
                                        node_postprocessors=[PageSortNodePostprocessor()],
                                        response_mode="accumulate",
                                        )

accumulate_fusion_sort_detail_engine = change_accumulate_engine_prompt_to_in_detail(
                                                        accumulate_fusion_sort_engine
                                                        )

# Create suammry retriever and engine
(summary_retriever,
summary_tree_detail_engine) = get_summary_retriever_and_tree_detail_engine(
                                                                    storage_context_summary
                                                                    )


# Create vector filter retreiver and engine
vector_filter_retriever = vector_index.as_retriever(
                                similarity_top_k=similarity_top_k,
                                filters=MetadataFilters.from_dicts(
                                    [{
                                        "key": "source", 
                                        # "value": "11",
                                        "value": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11"],
                                        "operator": "in"
                                        # "operator": "<="
                                    }]
                                )
                            )

vector_tree_filter_sort_engine = RetrieverQueryEngine.from_args(
    retriever=vector_filter_retriever, 
    response_mode="tree_summarize",
    node_postprocessors=[PageSortNodePostprocessor()],
    )

vector_tree_filter_sort_detail_engine = change_tree_engine_prompt_to_in_detail(
                                                            vector_tree_filter_sort_engine
                                                            )


# Add function tools
add_tool = FunctionTool.from_defaults(fn=add)
mystery_tool = FunctionTool.from_defaults(fn=mystery)


# The code below does not work (cannot put node_postprocessors here)
# window_retriever = index.as_retriever(
#     similarity_top_k=similarity_top_k,
#     node_postprocessors=[postproc],
#     )  

# query_str = "What are the keys to building a career in AI?"
# query_str = "What are the things that happen in New York?"
# query_str = "What are the things that are mentioned about Sam Altman?"
# query_str = "What are the things that are mentioned about startups?"
# query_str = "What are told about YC (Y Combinator)?"
# query_str = "What is the summary of the paul graham essay?"
# query_str = "What is the summary of the MetaGPT paper?"
# query_str = "How do agents share information with other agents?"
# query_str = "Tell me about the ablation study results."
query_str = "Tell me about the school days of the author of this essay."
# query_str = "Tell me about the early days of the author of this essay."
# query_str = "Who have been the president of YC (Y Combinator)?"
# query_str = "What are the thinkgs happened in New York in detail?"
# query_str = "What happened in New York?"
# query_str = "Describe everything that is mentioned about Interleaf one by one."
# query_str = "Describe everything that is mentioned about Interleaf."
# query_str = "Describe everything that is mentioned about Viaweb."
# query_str = "What happened at Interleaf?"
# query_str = "What happened at Interleaf and Viaweb?"
# query_str = "What is the importance of networking in AI?"
# query_str = (
#     "What could be the potential outcomes of adjusting the amount of safety"
#     " data used in the RLHF stage?"
# )
# summarize_query_str = "What is the summary of the paul graham essay?"


# Define window post processor
# postproc = MetadataReplacementPostProcessor(
#     target_metadata_key="window"
#     )

vector_store.client.load_collection(collection_name=collection_name_vector)

# # Retrieve nodes, bm25 nodes, and fusion nodes
# nodes = vector_retriever.retrieve(query_str)
# bm25_nodes = bm25_retriever.retrieve(query_str)
# fusion_nodes = fusion_retriever.retrieve(query_str)

# summary_nodes = summary_retriever.retrieve(summarize_query_str)

# # Define reranker
# rerank_model = "BAAI/bge-reranker-base"
# rerank = SentenceTransformerRerank(
#     top_n=rerank_top_n,
#     model=rerank_model,
#     )

# # Get re-ranked fusion nodes
# rerank_nodes = rerank.postprocess_nodes(
#     nodes=fusion_nodes,
#     query_str=query_str,
#     )

# Print retrieved nodes
# print_retreived_nodes("vector", nodes)
# print_retreived_nodes("bm25", bm25_nodes)
# print_retreived_nodes("fusion", fusion_nodes)
# print_retreived_nodes("rerank", rerank_nodes)
# print_retreived_nodes("summary", summary_nodes)


# print(sort_response.get_formatted_sources(length=2000))

# # Create default rerank engine (query_mode="compact")
# rerank_engine = RetrieverQueryEngine.from_args(
#     retriever=fusion_retriever, 
#     node_postprocessors=[rerank],
#     )

# rerank_sort_engine = RetrieverQueryEngine.from_args(
#     retriever=fusion_retriever, 
#     node_postprocessors=[
#         rerank,
#         PageSortNodePostprocessor()
#         ],
#     )

# # Create tree rerank engine (query_mode="compact")
# tree_rerank_engine = RetrieverQueryEngine.from_args(
#     retriever=fusion_retriever, 
#     node_postprocessors=[rerank],
#     response_mode="tree_summarize"
#     )

# tree_rerank_sort_engine = RetrieverQueryEngine.from_args(
#     retriever=fusion_retriever, 
#     node_postprocessors=[
#         rerank,
#         PageSortNodePostprocessor()
#         ],
#     response_mode="tree_summarize"
#     )

# accumulate_rerank_sort_engine = RetrieverQueryEngine.from_args(
#     retriever=fusion_retriever, 
#     node_postprocessors=[
#         rerank,
#         PageSortNodePostprocessor()
#         ],
#     response_mode="accumulate",
#     )

# # Create refine, tree, and accumulate rerank engines 
# (refine_rerank_engine, 
#  tree_rerank_engine, 
#  accumulate_rerank_engine) = get_rerank_refine_tree_and_accumulate_engine_from_retriever(
#                                                                         fusion_retriever,
#                                                                         rerank_top_n,
#                                                                         rerank
#                                                                         )

# summary_prompts_dict = tree_summary_engin.get_prompts()
# type = "tree_summary_engine:"
# display_prompt_dict(type.upper(), summary_prompts_dict)

# # Get default responses
# vector_response = vector_engine.query(query_str)
# bm25_response = bm25_engine.query(query_str)
# fusion_response = fusion_engine.query(query_str)
# rerank_response = rerank_engine.query(query_str)

# # Get default sorted responses
# vector_sort_response = vector_sort_engine.query(query_str)
# bm25_sort_response = bm25_sort_engine.query(query_str)
# fusion_sort_response = fusion_sort_engine.query(query_str)
# rerank_sort_response = rerank_sort_engine.query(query_str)

# Get tree responses
# tree_vector_response = tree_vector_engine.query(query_str)
# tree_bm25_response = tree_bm25_engine.query(query_str)
# tree_fusion_response = tree_fusion_engine.query(query_str)
# tree_rerank_response = tree_rerank_engine.query(query_str)

# Get tree sorted responses
# tree_vector_sort_response = tree_vector_sort_engine.query(query_str)
# tree_bm25_sort_response = tree_bm25_sort_engine.query(query_str)
# tree_fusion_sort_response = tree_fusion_sort_engine.query(query_str)
# tree_rerank_sort_response = tree_rerank_sort_engine.query(query_str)

# Get accumulate sorted responses
# accumulate_fusion_sort_response = accumulate_fusion_sort_engine.query(query_str)
# accumulate_rerank_sort_response = accumulate_rerank_sort_engine.query(query_str)

# tree_summary_engin_response = summary_tree_detail_engine.query(summarize_query_str)

# print(rerank_response.get_formatted_sources(length=2000))



filtered_nodes = vector_filter_retriever.retrieve(query_str)
filtered_nodes = [node.node for node in filtered_nodes ]  # get TextNode from ScoredNode

# Create bm25 filter engine
bm25_filter_retriever = BM25Retriever.from_defaults(
                            similarity_top_k=similarity_top_k,
                            nodes=filtered_nodes,
                            )

# Create fusion filter retreiver and engine
fusion_filter_retriever = QueryFusionRetriever(
                            retrievers=[
                                    vector_filter_retriever, 
                                    bm25_filter_retriever
                                    ],
                            similarity_top_k=fusion_top_n,
                            num_queries=num_queries,  # set this to 1 to disable query generation
                            mode="reciprocal_rerank",
                            use_async=True,
                            verbose=True,
                            # query_gen_prompt="...",  # for overriding the query generation prompt
                            )

fusion_accumulate_filter_sort_engine = RetrieverQueryEngine.from_args(
                                        retriever=fusion_filter_retriever, 
                                        node_postprocessors=[PageSortNodePostprocessor()],
                                        response_mode="accumulate",
                                        )

fusion_accumulate_filter_sort_detail_engine = change_accumulate_engine_prompt_to_in_detail(
                                                        fusion_accumulate_filter_sort_engine
                                                        )


# filter vector response
response_vector_filter = vector_tree_filter_sort_detail_engine.query(
                                                        query_str
                                                    )
print(str(response_vector_filter))

for n in response_vector_filter.source_nodes:
    print(n.metadata)


# filter fusion response
response_fusion_filter = fusion_accumulate_filter_sort_detail_engine.query(
                                                        query_str
                                                        )
print(str(response_fusion_filter))

for n in response_fusion_filter.source_nodes:
    print(n.metadata)



response_tool = llm.predict_and_call(
                            [add_tool, mystery_tool], 
                            "Tell me the output of the mystery function on 2 and 9", 
                            verbose=True
                            )
print(str(response_tool))


# Define query engine tools

# paul_summary = "Useful for summarization questions related to the paul graham essay."
# metagpt_summary = "Useful for summarization questions related to the MetaGPT paper."

summary_tool = QueryEngineTool.from_defaults(
    query_engine=summary_tree_detail_engine,
    description=(
        "Useful for summarization questions related to the documnet."
    ),
)

# paul_specific = "Useful for retrieving specific context from the paul graham essay."
# metagpt_specific = "Useful for retrieving specific context from the MetaGPT paper."

vector_tool = QueryEngineTool.from_defaults(
    query_engine=accumulate_fusion_sort_detail_engine,
    description=(
        "Useful for retrieving specific context from the document."
    ),
)

# Define tool query engine
query_engine = RouterQueryEngine(
    selector=LLMSingleSelector.from_defaults(),
    query_engine_tools=[
        summary_tool,
        vector_tool,
    ],
    verbose=True
)

response = query_engine.query(query_str)


# Prints responses
# print("\nVECTOR-ENGINE:\n\n" + str(vector_response))
# print("\nVECTOR-SORT-ENGINE:\n\n" + str(vector_sort_response))
# print("\nTREE-VECTOR-ENGINE:\n\n" + str(tree_vector_response))
# print("\nTREE-VECTOR-SORT-ENGINE:\n\n" + str(tree_vector_sort_response))

# print("\nBM25-ENGINE:\n\n" + str(bm25_response))
# print("\nBM25-SORT-ENGINE:\n\n" + str(bm25_sort_response))
# print("\nTREE-BM25-ENGINE:\n\n" + str(tree_bm25_response))
# print("\nTREE-BM25-SORT-ENGINE:\n\n" + str(tree_bm25_sort_response))

# print("\nFUSION-ENGINE:\n\n" + str(fusion_response))
# print("\nFUSION-SORT-ENGINE:\n\n" + str(fusion_sort_response))
# print("\nTREE-FUSION-ENGINE:\n\n" + str(tree_fusion_response))
# print("\nTREE-FUSION-SORT-ENGINE:\n\n" + str(tree_fusion_sort_response))

# print("\nRERANK-ENGINE:\n\n" + str(rerank_response))
# print("\nRERANK-SORT-ENGINE:\n\n" + str(rerank_sort_response))
# print("\nTREE-RERANK-ENGINE:\n\n" + str(tree_rerank_response))
# print("\nTREE-RERANK-SORT-ENGINE:\n\n" + str(tree_rerank_sort_response))

# print("\nACCUMULATE-FUSION-SORT-ENGINE:\n\n" + str(accumulate_fusion_sort_response))
# print("\nACCUMULATE-RERANK-SORT-ENGINE:\n\n" + str(accumulate_rerank_sort_response))

# print("\nTREE-SUMMARY-ENGINE:\n\n" + str(tree_summary_engin_response))

print("\nTOOL-ENGINE:\n\n" + str(response))

for n in response.source_nodes:
    print(n.metadata)

vector_store.client.release_collection(collection_name=collection_name_vector)
vector_store.client.close()  
# Need to do this (otherwise Milvus container will hault when closing)
# del docstore  # MongoDB may frnot need to be manually closed.






